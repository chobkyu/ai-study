from transformers import BlipProcessor, BlipForQuestionAnswering
from PIL import Image

# ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
model_name = "Salesforce/blip-vqa-base"
processor = BlipProcessor.from_pretrained(model_name)
model = BlipForQuestionAnswering.from_pretrained(model_name)

# í…ŒìŠ¤íŠ¸ ìš© ì´ë¯¸ì§€ì™€ ì§ˆë¬¸
image_path = "cat.26.jpg"
question = "ê³ ì–‘ì´ê°€ ì¡´ì¬í•˜ë‚˜ìš”?"

# ì´ë¯¸ì§€ ë¡œë“œ
image = Image.open(image_path).convert("RGB")

# ëª¨ë¸ ì…ë ¥ ì¤€ë¹„
inputs = processor(image, question, return_tensors="pt")

# ëª¨ë¸ ì¶”ë¡  (ë‹µë³€ ìƒì„±)
out = model.generate(**inputs, max_new_tokens=20)
answer = processor.decode(out[0], skip_special_tokens=True)

print(f"â“ ì§ˆë¬¸: {question}")
print(f"ğŸ’¬ ë‹µë³€: {answer}")